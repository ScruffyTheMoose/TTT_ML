{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "from TicTacToe import Board, GameTools as gt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing the neural network multiclassifier model.\n",
    "\n",
    "This particular model has 3 hidden layers, with each layer containing 30 neurons. The activation function for the hidden layers is set to be ReLu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-8 {color: black;background-color: white;}#sk-container-id-8 pre{padding: 0;}#sk-container-id-8 div.sk-toggleable {background-color: white;}#sk-container-id-8 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-8 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-8 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-8 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-8 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-8 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-8 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-8 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-8 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-8 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-8 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-8 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-8 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-8 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-8 div.sk-item {position: relative;z-index: 1;}#sk-container-id-8 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-8 div.sk-item::before, #sk-container-id-8 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-8 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-8 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-8 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-8 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-8 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-8 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-8 div.sk-label-container {text-align: center;}#sk-container-id-8 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-8 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-8\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(hidden_layer_sizes=(30, 30, 30))</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" checked><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(hidden_layer_sizes=(30, 30, 30))</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MLPClassifier(hidden_layer_sizes=(30, 30, 30))"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent = MLPClassifier(hidden_layer_sizes=(30, 30, 30), activation='relu')\n",
    "agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demonstrating how to use the partial_fit() method.\n",
    "\n",
    "You must provide a matrix of features, an array of labels, and an array of all classes that your model may need to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.5767843886770314"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = np.array([[0, 0, 0, 0, 0, 0, 0, 0, 0]]) # 2d array since features are always in a matrix format\n",
    "label = np.array([5])\n",
    "classes = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "\n",
    "agent.partial_fit(X=features, y=label, classes=classes)\n",
    "agent.loss_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write a function that competes our agent against a random player.\n",
    "\n",
    "In order to keep this demonstration as simple as possible, we will build this training environment with a limited number of settings in the parameters. You may want to consider adding additional parameters so you can adjust how your model interacts with the environment.\n",
    "\n",
    "Be very careful of what you code into the training environment. It's always a good idea to hand trace a game or two in your environment to make sure you aren't feeding your model erroneous data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def environment(model: MLPClassifier, model_plays_first: bool, training_type: str = \"dense\", train_model: bool = False, proba_threshold: float = 1.0, threshold_type: str = \"lower\", drop_multiplier: float = 0.0, drop_constant: float = 0.0, print_board: bool = False):\n",
    "    game = Board() # initializing a new game of TTT\n",
    "    \n",
    "    match_states = list() # a list to store the state of the board for each play made by our model\n",
    "    match_labels = list() # a list to store the labels associated with each match state\n",
    "    alt_labels = list() # a list to store alternative labels for the case where our model loses\n",
    "    \n",
    "    status = game.status() # function specific to TTT. Will be used to check if the game has ended\n",
    "    \n",
    "    trained = False # to determine if our model recieved training during a given match\n",
    "    \n",
    "    moves_remaining = 9\n",
    "    \n",
    "    # while loop which plays a game of TTT\n",
    "    while moves_remaining > 0 and not status[0]:\n",
    "        state = deepcopy(game.vector) # getting copy of the board state in a vectorized format - Ex: [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "\n",
    "        label = None\n",
    "        \n",
    "        coords = gt.avail_moves(game.board) # using a tool specific to our TTT game to get the (row, column) coordinates for available positions on the board\n",
    "\n",
    "        matrix_state = np.array([state]) # putting our state into a numpy matrix. we need to use a consistent data structure and format for training the model and numpy works well\n",
    "        \n",
    "        # if model is playing first and it is first player's turn\n",
    "        # this is the model's move\n",
    "        if model_plays_first and moves_remaining % 2 != 0:\n",
    "            prediction = model.predict(matrix_state)[0] # predicting the best move based on the current game board\n",
    "            proba = model.predict_proba(matrix_state)[0][prediction - 1]\n",
    "            \n",
    "            move = ((prediction - 1) // 3, (prediction - 1) % 3) # converting an integer into (row, column) coordinates. This is specific to our TTT\n",
    "            # it is possible the model will try to make a move that isn't possible because the space is taken. We will validate that the move was made successfully here.\n",
    "            out = game.move(row=move[0], column=move[1], player=1) # move() function returns True if move was successful, false otherwise\n",
    "            \n",
    "            if out:\n",
    "                label = prediction\n",
    "            else:\n",
    "                move = random.choice(coords) # choosing randomly from known available positions\n",
    "                coords.remove(move) # dropping our selected move from the available positions on the board\n",
    "                \n",
    "                label = (3 * move[0]) + move[1] + 1 # converting coordinates to integer value\n",
    "                game.move(row=move[0], column=move[1], player=1) # placing random move on the board\n",
    "                \n",
    "            if (proba < proba_threshold and threshold_type == \"lower\") or (proba > proba_threshold and threshold_type == \"upper\"): # we only add this data to the training set if it does not meet the threshold\n",
    "                match_states.append(state) # saving state\n",
    "                match_labels.append(label) # saving associated label\n",
    "            \n",
    "                if len(coords) > 0:\n",
    "                    move = random.choice(coords)\n",
    "                    pos = (3 * move[0]) + move[1] + 1\n",
    "                    alt_labels.append(pos) # saving alternate label\n",
    "                else:\n",
    "                    alt_labels.append(label) # there was no alternative because all other positions were taken\n",
    "                    \n",
    "        # if model is playing second and it is first player's turn\n",
    "        # this is the random players move\n",
    "        elif not model_plays_first and moves_remaining % 2 != 0:\n",
    "            move = random.choice(coords)\n",
    "            game.move(row=move[0], column=move[1], player=-1)\n",
    "        \n",
    "        # if model is playing first and it is the second player's turn\n",
    "        # this is the random players move\n",
    "        elif model_plays_first and moves_remaining % 2 == 0:\n",
    "            move = random.choice(coords)\n",
    "            game.move(row=move[0], column=move[1], player=-1)\n",
    "            \n",
    "        # if model is playing second and it is the second player's turn\n",
    "        # this is the model's move\n",
    "        else:\n",
    "            prediction = model.predict(matrix_state)[0] # predicting the best move based on the current game board\n",
    "            proba = model.predict_proba(matrix_state)[0][prediction - 1] # getting the probability to determine how confident the agent is\n",
    "            \n",
    "            move = ((prediction - 1) // 3, (prediction - 1) % 3) # converting an integer into (row, column) coordinates. This is specific to our TTT\n",
    "            \n",
    "            # it is possible the model will try to make a move that isn't possible because the space is taken. We will validate that the move was made successfully here.\n",
    "            out = game.move(row=move[0], column=move[1], player=1) # move() function returns True if move was successful, false otherwise\n",
    "            \n",
    "            if out:\n",
    "                label = prediction\n",
    "            else:\n",
    "                move = random.choice(coords) # choosing randomly from known available positions\n",
    "                coords.remove(move) # dropping our selected move from the available positions on the board\n",
    "                \n",
    "                label = (3 * move[0]) + move[1] + 1 # converting coordinates to integer value\n",
    "                game.move(row=move[0], column=move[1], player=1) # placing random move on the board\n",
    "                \n",
    "            if (proba < proba_threshold and threshold_type == \"lower\") or (proba > proba_threshold and threshold_type == \"upper\"): # we only add this data to the training set if it does not meet the threshold\n",
    "                match_states.append(state) # saving state\n",
    "                match_labels.append(label) # saving associated label\n",
    "            \n",
    "                if len(coords) > 0:\n",
    "                    move = random.choice(coords)\n",
    "                    pos = (3 * move[0]) + move[1] + 1\n",
    "                    alt_labels.append(pos) # saving alternate label\n",
    "                else:\n",
    "                    alt_labels.append(label) # there was no alternative because all other positions were taken\n",
    "        \n",
    "        \n",
    "        moves_remaining -= 1 # decrementing the remaining move count\n",
    "        status = game.status() # updating the status of the game\n",
    "        \n",
    "    # end of the while loop for playing a game of TTT\n",
    "    \n",
    "    # if our setting for training the model is set to True, then the model will be partially fit to the data from this game\n",
    "    # if set to false, this environment just allows a model to compete against a random player to evaluate performance\n",
    "    # also checking that match_states is not an empty list before going into drops\n",
    "    if train_model and match_states:\n",
    "        \n",
    "        # dropping training data based on how early in the game it was\n",
    "        # starting at 0.0, each subsequent sample has an additional 0.2 chance of being kept as training data\n",
    "        drop_indexes = list()\n",
    "        for n in range(len(match_states)):\n",
    "            if random.random() > (n * drop_multiplier + drop_constant):\n",
    "                drop_indexes.append(n)\n",
    "                \n",
    "        for idx in drop_indexes[::-1]:\n",
    "            match_states.pop(idx)\n",
    "            match_labels.pop(idx)\n",
    "            alt_labels.pop(idx)\n",
    "            \n",
    "        if match_states:\n",
    "            trained = True\n",
    "            # converting our features, the match states, into a numpy matrix\n",
    "            features = np.array(match_states)\n",
    "            \n",
    "            # converting our labels into a numpy array\n",
    "            # if the model won, we use the winning labels\n",
    "            # if the model lost, we use the alternate labels\n",
    "            if status[1] == 1:\n",
    "                labels = np.array(match_labels)\n",
    "            else:\n",
    "                labels = np.array(alt_labels)\n",
    "            \n",
    "            # a list explicitly stating the possible classes for our model - a required parameter for partial fitting\n",
    "            classes = [x for x in range(1, 10)]\n",
    "        \n",
    "            if training_type == \"sparse_winner\" and status[1] == 1:\n",
    "                model.partial_fit(features, labels, classes)\n",
    "                \n",
    "            elif training_type == \"sparse_loser\" and status[1] == -1:\n",
    "                model.partial_fit(features, labels, classes)\n",
    "                \n",
    "            elif training_type == \"dense\":\n",
    "                model.partial_fit(features, labels, classes)\n",
    "    \n",
    "    # if print_board set to true, will print a nice string formatting version of the board\n",
    "    if print_board:\n",
    "        print(game.print())\n",
    "    \n",
    "    return status[1], trained, match_states, match_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a fully functional training environment for our agent.\n",
    "\n",
    "We can run a single match without training and examine the output from our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " X |   |   \n",
      "-----------\n",
      " X | O |   \n",
      "-----------\n",
      " X | O |   \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1,\n",
       " False,\n",
       " [[0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 1, -1, 0],\n",
       "  [1, 0, 0, 0, -1, 0, 1, -1, 0]],\n",
       " [7, 1, 4])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = environment(model=agent, model_plays_first=True, train_model=False, print_board=True)\n",
    "t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like our model is successfully playing a game of Tic Tac Toe! We can also see that our environment is able to successfully track all of the game states and decisions made by our model. This is good news!\n",
    "\n",
    "Now, we can iteratively train the model and then see how it performs. In this case, we set the model to only play as the second player."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3322"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_passes = 0\n",
    "# 10000 training iterations\n",
    "for _ in range(10000):\n",
    "    # Most Effective parameters so far:\n",
    "    # model=agent, model_plays_first=False, train_model=True, training_type=\"dense\", proba_threshold=0.5, threshold_type=\"lower\")\n",
    "    t = environment(model=agent, model_plays_first=False, train_model=True, training_type=\"dense\", proba_threshold=0.5, threshold_type=\"lower\", drop_multiplier=0.1, drop_constant=0.05) \n",
    "    if t[1]:\n",
    "        training_passes += 1\n",
    "        \n",
    "training_passes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our agent has now been trained on 10000 matches of TTT.\n",
    "\n",
    "We can ask our agent to predict the best move from a random board as a test of its functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1, 2, 3, 4, 5, 6, 7, 8, 9]), 0.5108077194125298, array([5]))"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_board = np.array([[1, 0, 0, 0, 1, 0, -1, -1, 0]])\n",
    "zero_board = np.array([[0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
    "\n",
    "pred = agent.predict(zero_board)\n",
    "\n",
    "agent.classes_, agent.predict_proba(zero_board)[0][pred - 1][0], pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now put our agent into matches against a random player and evaluate how the agent performs. Notice that the \"train_model\" parameter is set to \"False\". Our agent will strictly play, it will not learn.\n",
    "\n",
    "We will observe its performance when it plays first and when it plays second.\n",
    "\n",
    "Anything other than a win is considered a loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8630, 1036, 334)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "win_ct = 0\n",
    "loss_ct = 0\n",
    "draw_ct = 0\n",
    "\n",
    "for _ in range(5000):\n",
    "    f = environment(model=agent, model_plays_first=True, train_model=False, print_board=False)\n",
    "    \n",
    "    if f[0] == 1:\n",
    "        win_ct += 1\n",
    "    elif f[0] == 0:\n",
    "        draw_ct += 1\n",
    "    else:\n",
    "        loss_ct += 1\n",
    "    \n",
    "for _ in range(5000):\n",
    "    s = environment(model=agent, model_plays_first=False, train_model=False, print_board=False)\n",
    "\n",
    "    if s[0] == 1:\n",
    "        win_ct += 1\n",
    "    elif s[0] == 0:\n",
    "        draw_ct += 1\n",
    "    else:\n",
    "        loss_ct += 1\n",
    "        \n",
    "win_ct, loss_ct, draw_ct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a functioning environment for our agent to play against a random opponent, let's build another environment where we can compete models against eachother to further advance their skill."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comp_environment(model: MLPClassifier, opponent: MLPClassifier, model_plays_first: bool, training_type: str = \"dense\", train_model: bool = False, proba_threshold: float = 1.0, threshold_type: str = \"lower\", drop_multiplier: float = 0.0, drop_constant: float = 0.0, print_board: bool = False):\n",
    "    game = Board() # initializing a new game of TTT\n",
    "    \n",
    "    match_states = list() # a list to store the state of the board for each play made by our model\n",
    "    match_labels = list() # a list to store the labels associated with each match state\n",
    "    alt_labels = list() # a list to store alternative labels for the case where our model loses\n",
    "    \n",
    "    status = game.status() # function specific to TTT. Will be used to check if the game has ended\n",
    "    \n",
    "    trained = False # to determine if our model recieved training during a given match\n",
    "    \n",
    "    moves_remaining = 9\n",
    "    \n",
    "    # while loop which plays a game of TTT\n",
    "    while moves_remaining > 0 and not status[0]:\n",
    "        state = deepcopy(game.vector) # getting copy of the board state in a vectorized format - Ex: [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "\n",
    "        label = None\n",
    "        \n",
    "        coords = gt.avail_moves(game.board) # using a tool specific to our TTT game to get the (row, column) coordinates for available positions on the board\n",
    "\n",
    "        matrix_state = np.array([state]) # putting our state into a numpy matrix. we need to use a consistent data structure and format for training the model and numpy works well\n",
    "        \n",
    "        # if model is playing first and it is first player's turn\n",
    "        # this is the model's move\n",
    "        if model_plays_first and moves_remaining % 2 != 0:\n",
    "            prediction = model.predict(matrix_state)[0] # predicting the best move based on the current game board\n",
    "            proba = model.predict_proba(matrix_state)[0][prediction - 1]\n",
    "            \n",
    "            move = ((prediction - 1) // 3, (prediction - 1) % 3) # converting an integer into (row, column) coordinates. This is specific to our TTT\n",
    "            # it is possible the model will try to make a move that isn't possible because the space is taken. We will validate that the move was made successfully here.\n",
    "            out = game.move(row=move[0], column=move[1], player=1) # move() function returns True if move was successful, false otherwise\n",
    "            \n",
    "            if out:\n",
    "                label = prediction\n",
    "            else:\n",
    "                move = random.choice(coords) # choosing randomly from known available positions\n",
    "                coords.remove(move) # dropping our selected move from the available positions on the board\n",
    "                \n",
    "                label = (3 * move[0]) + move[1] + 1 # converting coordinates to integer value\n",
    "                game.move(row=move[0], column=move[1], player=1) # placing random move on the board\n",
    "                \n",
    "            if (proba < proba_threshold and threshold_type == \"lower\") or (proba > proba_threshold and threshold_type == \"upper\"): # we only add this data to the training set if it does not meet the threshold\n",
    "                match_states.append(state) # saving state\n",
    "                match_labels.append(label) # saving associated label\n",
    "            \n",
    "                if len(coords) > 0:\n",
    "                    move = random.choice(coords)\n",
    "                    pos = (3 * move[0]) + move[1] + 1\n",
    "                    alt_labels.append(pos) # saving alternate label\n",
    "                else:\n",
    "                    alt_labels.append(label) # there was no alternative because all other positions were taken\n",
    "                    \n",
    "        # if model is playing second and it is first player's turn\n",
    "        # this is the opponent's move\n",
    "        elif not model_plays_first and moves_remaining % 2 != 0:\n",
    "            prediction = opponent.predict(matrix_state)[0] # predicting the best move based on the current game board\n",
    "            proba = opponent.predict_proba(matrix_state)[0][prediction - 1]\n",
    "            \n",
    "            move = ((prediction - 1) // 3, (prediction - 1) % 3) # converting an integer into (row, column) coordinates. This is specific to our TTT\n",
    "            # it is possible the model will try to make a move that isn't possible because the space is taken. We will validate that the move was made successfully here.\n",
    "            out = game.move(row=move[0], column=move[1], player=-1) # move() function returns True if move was successful, false otherwise\n",
    "            \n",
    "            if not out:\n",
    "                move = random.choice(coords) # choosing randomly from known available positions\n",
    "                coords.remove(move) # dropping our selected move from the available positions on the board\n",
    "                \n",
    "                game.move(row=move[0], column=move[1], player=-1) # placing random move on the board\n",
    "        \n",
    "        # if model is playing first and it is the second player's turn\n",
    "        # this is the opponent's move\n",
    "        elif model_plays_first and moves_remaining % 2 == 0:\n",
    "            prediction = opponent.predict(matrix_state)[0] # predicting the best move based on the current game board\n",
    "            proba = opponent.predict_proba(matrix_state)[0][prediction - 1]\n",
    "            \n",
    "            move = ((prediction - 1) // 3, (prediction - 1) % 3) # converting an integer into (row, column) coordinates. This is specific to our TTT\n",
    "            # it is possible the model will try to make a move that isn't possible because the space is taken. We will validate that the move was made successfully here.\n",
    "            out = game.move(row=move[0], column=move[1], player=-1) # move() function returns True if move was successful, false otherwise\n",
    "            \n",
    "            if not out:\n",
    "                move = random.choice(coords) # choosing randomly from known available positions\n",
    "                coords.remove(move) # dropping our selected move from the available positions on the board\n",
    "                \n",
    "                game.move(row=move[0], column=move[1], player=-1) # placing random move on the board\n",
    "            \n",
    "        # if model is playing second and it is the second player's turn\n",
    "        # this is the model's move\n",
    "        else:\n",
    "            prediction = model.predict(matrix_state)[0] # predicting the best move based on the current game board\n",
    "            proba = model.predict_proba(matrix_state)[0][prediction - 1] # getting the probability to determine how confident the agent is\n",
    "            \n",
    "            move = ((prediction - 1) // 3, (prediction - 1) % 3) # converting an integer into (row, column) coordinates. This is specific to our TTT\n",
    "            \n",
    "            # it is possible the model will try to make a move that isn't possible because the space is taken. We will validate that the move was made successfully here.\n",
    "            out = game.move(row=move[0], column=move[1], player=1) # move() function returns True if move was successful, false otherwise\n",
    "            \n",
    "            if out:\n",
    "                label = prediction\n",
    "            else:\n",
    "                move = random.choice(coords) # choosing randomly from known available positions\n",
    "                coords.remove(move) # dropping our selected move from the available positions on the board\n",
    "                \n",
    "                label = (3 * move[0]) + move[1] + 1 # converting coordinates to integer value\n",
    "                game.move(row=move[0], column=move[1], player=1) # placing random move on the board\n",
    "                \n",
    "            if (proba < proba_threshold and threshold_type == \"lower\") or (proba > proba_threshold and threshold_type == \"upper\"): # we only add this data to the training set if it does not meet the threshold\n",
    "                match_states.append(state) # saving state\n",
    "                match_labels.append(label) # saving associated label\n",
    "            \n",
    "                if len(coords) > 0:\n",
    "                    move = random.choice(coords)\n",
    "                    pos = (3 * move[0]) + move[1] + 1\n",
    "                    alt_labels.append(pos) # saving alternate label\n",
    "                else:\n",
    "                    alt_labels.append(label) # there was no alternative because all other positions were taken\n",
    "        \n",
    "        \n",
    "        moves_remaining -= 1 # decrementing the remaining move count\n",
    "        status = game.status() # updating the status of the game\n",
    "        \n",
    "    # end of the while loop for playing a game of TTT\n",
    "    \n",
    "    # if our setting for training the model is set to True, then the model will be partially fit to the data from this game\n",
    "    # if set to false, this environment just allows a model to compete against a random player to evaluate performance\n",
    "    # also checking that match_states is not an empty list\n",
    "    if train_model and match_states:\n",
    "        \n",
    "        # dropping training data based on how early in the game it was\n",
    "        # starting at 0.0, each subsequent sample has an additional 0.2 chance of being kept as training data\n",
    "        drop_indexes = list()\n",
    "        for n in range(len(match_states)):\n",
    "            if random.random() > (n * drop_multiplier + drop_constant):\n",
    "                drop_indexes.append(n)\n",
    "                \n",
    "        for idx in drop_indexes[::-1]:\n",
    "            match_states.pop(idx)\n",
    "            match_labels.pop(idx)\n",
    "            alt_labels.pop(idx)\n",
    "            \n",
    "        if match_states:\n",
    "            trained = True\n",
    "            # converting our features, the match states, into a numpy matrix\n",
    "            features = np.array(match_states)\n",
    "            \n",
    "            # converting our labels into a numpy array\n",
    "            # if the model won, we use the winning labels\n",
    "            # if the model lost, we use the alternate labels\n",
    "            if status[1] == 1:\n",
    "                labels = np.array(match_labels)\n",
    "            else:\n",
    "                labels = np.array(alt_labels)\n",
    "            \n",
    "            # a list explicitly stating the possible classes for our model - a required parameter for partial fitting\n",
    "            classes = [x for x in range(1, 10)]\n",
    "        \n",
    "            if training_type == \"sparse_winner\" and status[1] == 1:\n",
    "                model.partial_fit(features, labels, classes)\n",
    "                \n",
    "            elif training_type == \"sparse_loser\" and status[1] == -1:\n",
    "                model.partial_fit(features, labels, classes)\n",
    "                \n",
    "            elif training_type == \"dense\":\n",
    "                model.partial_fit(features, labels, classes)\n",
    "    \n",
    "    # if print_board set to true, will print a nice string formatting version of the board\n",
    "    if print_board:\n",
    "        print(game.print())\n",
    "    \n",
    "    return status[1], trained, match_states, match_labels, alt_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent_main = pickle.load(open(\"model_binaries/demo_agent_main.sav\", \"rb\"))\n",
    "agent_secondary = pickle.load(open(\"model_binaries/demo_agent_secondary.sav\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passes: 2267 wins: 576 losses: 6226 draws: 3198\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(6708, 2954, 338)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_wins = 0\n",
    "training_loss = 0\n",
    "training_draw = 0\n",
    "passes = 0\n",
    "\n",
    "for _ in range(10000):\n",
    "    t = comp_environment(model=agent, opponent=agent_secondary, model_plays_first=False, train_model=True, training_type=\"sparse_winner\", proba_threshold=1, threshold_type=\"lower\", drop_multiplier=0.05, drop_constant=0.0, print_board=False)\n",
    "    if t[0] == 1:\n",
    "        training_wins += 1\n",
    "    elif t[0] == 0:\n",
    "        training_draw += 1\n",
    "    else:\n",
    "        training_loss += 1\n",
    "    \n",
    "    if t[1]:\n",
    "        passes += 1\n",
    "\n",
    "print(\"Passes:\", passes, \"wins:\", training_wins, \"losses:\", training_loss, \"draws:\", training_draw)\n",
    "    \n",
    "win_ct = 0\n",
    "loss_ct = 0\n",
    "draw_ct = 0\n",
    "\n",
    "for _ in range(5000):\n",
    "    f = environment(model=agent, model_plays_first=True, train_model=False, print_board=False)\n",
    "    \n",
    "    if f[0] == 1:\n",
    "        win_ct += 1\n",
    "    elif f[0] == 0:\n",
    "        draw_ct += 1\n",
    "    else:\n",
    "        loss_ct += 1\n",
    "    \n",
    "for _ in range(5000):\n",
    "    s = environment(model=agent, model_plays_first=False, train_model=False, print_board=False)\n",
    "\n",
    "    if s[0] == 1:\n",
    "        win_ct += 1\n",
    "    elif s[0] == 0:\n",
    "        draw_ct += 1\n",
    "    else:\n",
    "        loss_ct += 1\n",
    "        \n",
    "win_ct, loss_ct, draw_ct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b8f6c3ebd6a88d6aa92e8e216424c33c840aaeca7e1202db6e523f3cae8f4bb3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
